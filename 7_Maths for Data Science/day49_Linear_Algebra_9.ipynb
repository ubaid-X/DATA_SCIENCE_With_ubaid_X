{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e8cd6f8",
   "metadata": {},
   "source": [
    "---\n",
    "__About Section:__\n",
    "\n",
    "- __Author name:__ UBAIDULLAH\n",
    "\n",
    "- __Email:__ [ai.bussiness.student0@gmail.com](mailto:ai.bussiness.student0@gmail.com)\n",
    "\n",
    "- __GitHub:__ [github.com/ubaid-X/](https://github.com/ubaid-X/)\n",
    "\n",
    "- __LinkedIn Profile:__ [linkedin.com/in/ubaid-ullah-634563373/](https://www.linkedin.com/in/ubaid-ullah-634563373/)\n",
    "\n",
    "- __Kaggle:__ [kaggle.com/ubaidullah01](https://www.kaggle.com/ubaidullah01)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c473c8",
   "metadata": {},
   "source": [
    "## 5. Gaussian Elimination\n",
    "\n",
    "### Definition\n",
    "A step-by-step method to simplify matrices until they look like stairs, making them easier to solve.\n",
    "\n",
    "### How It Works\n",
    "1. Write equations in an augmented matrix\n",
    "2. Use row operations to make zeros below the diagonal\n",
    "3. Solve from bottom to top (back substitution)\n",
    "\n",
    "### Example\n",
    "Let's solve:\n",
    "- `x + y + z = 6`\n",
    "- `2x + y + 3z = 14`\n",
    "- `x + 2y + z = 8`\n",
    "\n",
    "**Step 1:** Make augmented matrix:\n",
    "```\n",
    "[1  1  1 | 6]\n",
    "[2  1  3 |14]\n",
    "[1  2  1 | 8]\n",
    "```\n",
    "\n",
    "**Step 2:** Row operations:\n",
    "- R2 = R2 - 2×R1\n",
    "- R3 = R3 - R1\n",
    "\n",
    "New matrix:\n",
    "```\n",
    "[1  1  1 | 6]\n",
    "[0 -1  1 | 2]\n",
    "[0  1  0 | 2]\n",
    "```\n",
    "\n",
    "**Step 3:** More operations:\n",
    "- R3 = R3 + R2\n",
    "\n",
    "New matrix:\n",
    "```\n",
    "[1  1  1 | 6]\n",
    "[0 -1  1 | 2]\n",
    "[0  0  1 | 4]\n",
    "```\n",
    "\n",
    "**Step 4:** Back substitution:\n",
    "- From R3: z = 4\n",
    "- From R2: -y + 4 = 2 → -y = -2 → y = 2\n",
    "- From R1: x + 2 + 4 = 6 → x + 6 = 6 → x = 0\n",
    "\n",
    "**Answer:** x = 0, y = 2, z = 4\n",
    "\n",
    "### Benefits\n",
    "- Works for any system\n",
    "- Very systematic\n",
    "- Good for computers\n",
    "\n",
    "### Limitations\n",
    "- Many steps\n",
    "- Easy to make small mistakes\n",
    "- Can get messy with fractions\n",
    "\n",
    "### When to Use\n",
    "- For systems with many equations\n",
    "- When other methods don't work\n",
    "- When working without a calculator\n",
    "\n",
    "### Tips\n",
    "- Take it one step at a time\n",
    "- Double-check your arithmetic\n",
    "- Practice with simple systems first\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Gauss-Jordan Elimination\n",
    "\n",
    "### Definition\n",
    "An extension of Gaussian elimination that makes the matrix even simpler, so you can read the answers directly.\n",
    "\n",
    "### How It Works\n",
    "1. Do Gaussian elimination first\n",
    "2. Continue with row operations to get ones on diagonal\n",
    "3. Make zeros above the diagonal too\n",
    "4. Read answers directly from the matrix\n",
    "\n",
    "### Example\n",
    "Let's solve:\n",
    "- `2x + y = 5`\n",
    "- `x - y = 1`\n",
    "\n",
    "**Step 1:** Augmented matrix:\n",
    "```\n",
    "[2  1 | 5]\n",
    "[1 -1 | 1]\n",
    "```\n",
    "\n",
    "**Step 2:** Row operations:\n",
    "- Swap R1 and R2:\n",
    "```\n",
    "[1 -1 | 1]\n",
    "[2  1 | 5]\n",
    "```\n",
    "- R2 = R2 - 2×R1:\n",
    "```\n",
    "[1 -1 | 1]\n",
    "[0  3 | 3]\n",
    "```\n",
    "- R2 = R2 ÷ 3:\n",
    "```\n",
    "[1 -1 | 1]\n",
    "[0  1 | 1]\n",
    "```\n",
    "- R1 = R1 + R2:\n",
    "```\n",
    "[1  0 | 2]\n",
    "[0  1 | 1]\n",
    "```\n",
    "\n",
    "**Answer:** x = 2, y = 1 (directly from the matrix!)\n",
    "\n",
    "### Benefits\n",
    "- Gives answers directly\n",
    "- Very elegant\n",
    "- No back substitution needed\n",
    "\n",
    "### Limitations\n",
    "- Even more steps than Gaussian\n",
    "- More calculations\n",
    "- Can be time-consuming\n",
    "\n",
    "### When to Use\n",
    "- When you want the most reduced form\n",
    "- For theoretical work\n",
    "- When you need to see the inverse\n",
    "\n",
    "### Tips\n",
    "- Master Gaussian elimination first\n",
    "- Be patient with the steps\n",
    "- Check your final matrix carefully\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c842af0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 7. LU Decomposition Method\n",
    "\n",
    "## 1. Simple Definition\n",
    "\n",
    "LU Decomposition (Lower-Upper Decomposition) is a matrix factorization method that decomposes a square matrix into two triangular matrices:\n",
    "- **L** (Lower triangular matrix) with diagonal elements = 1\n",
    "- **U** (Upper triangular matrix)\n",
    "\n",
    "Such that: **A = L × U**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Explanation\n",
    "\n",
    "LU Decomposition is a fundamental technique in numerical linear algebra that:\n",
    "- Factorizes a square matrix into lower and upper triangular matrices\n",
    "- Provides an efficient way to solve systems of linear equations\n",
    "- Serves as the foundation for matrix inversion and determinant calculation\n",
    "- Is more computationally efficient than Gaussian elimination when solving multiple equations with the same coefficient matrix\n",
    "\n",
    "The decomposition works for any square matrix where the decomposition exists (all leading principal minors ≠ 0).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Example: Solve for x, y, z\n",
    "\n",
    "**System of Equations:**\n",
    "```\n",
    "2x + y + z = 5\n",
    "4x - 6y + 0z = -2\n",
    "-2x + 7y + 2z = 9\n",
    "```\n",
    "\n",
    "**Matrix form: A × X = B**\n",
    "```\n",
    "A = [[2, 1, 1],\n",
    "     [4, -6, 0],\n",
    "     [-2, 7, 2]]\n",
    "     \n",
    "B = [[5],\n",
    "     [-2],\n",
    "     [9]]\n",
    "```\n",
    "\n",
    "### Step 1: LU Decomposition of Matrix A\n",
    "\n",
    "**Initialize L and U:**\n",
    "```\n",
    "L = [[1, 0, 0],\n",
    "     [0, 1, 0],\n",
    "     [0, 0, 1]]\n",
    "\n",
    "U = [[2, 1, 1],\n",
    "     [4, -6, 0],\n",
    "     [-2, 7, 2]]\n",
    "```\n",
    "\n",
    "**Step 1.1: Eliminate first column below pivot (2)**\n",
    "- Multiplier for row2: 4/2 = 2\n",
    "- Multiplier for row3: -2/2 = -1\n",
    "\n",
    "Update:\n",
    "```\n",
    "U = [[2,  1,  1],\n",
    "     [0, -8, -2],  # Row2 - 2×Row1\n",
    "     [0,  8,  3]]  # Row3 - (-1)×Row1\n",
    "\n",
    "L = [[1, 0, 0],\n",
    "     [2, 1, 0],    # Store multiplier in L[1,0]\n",
    "     [-1, 0, 1]]   # Store multiplier in L[2,0]\n",
    "```\n",
    "\n",
    "**Step 1.2: Eliminate second column below pivot (-8)**\n",
    "- Multiplier for row3: 8/-8 = -1\n",
    "\n",
    "Update:\n",
    "```\n",
    "U = [[2,  1,  1],\n",
    "     [0, -8, -2],\n",
    "     [0,  0,  1]]  # Row3 - (-1)×Row2\n",
    "\n",
    "L = [[1,  0, 0],\n",
    "     [2,  1, 0],\n",
    "     [-1, -1, 1]]  # Store multiplier in L[2,1]\n",
    "```\n",
    "\n",
    "**Verification: L × U = A**\n",
    "```\n",
    "L × U = [[1,0,0][2,1,0][-1,-1,1]] × [[2,1,1][0,-8,-2][0,0,1]]\n",
    "       = [[2,1,1][4,-6,0][-2,7,2]] = A ✓\n",
    "```\n",
    "\n",
    "### Step 2: Solve L × Y = B for Y using forward substitution\n",
    "\n",
    "**System:**\n",
    "```\n",
    "1y₁ + 0y₂ + 0y₃ = 5\n",
    "2y₁ + 1y₂ + 0y₃ = -2\n",
    "-1y₁ -1y₂ + 1y₃ = 9\n",
    "```\n",
    "\n",
    "**Forward substitution:**\n",
    "- y₁ = 5/1 = 5\n",
    "- y₂ = (-2 - 2×5)/1 = -12\n",
    "- y₃ = (9 - (-1×5) - (-1×-12))/1 = (9 + 5 - 12) = 2\n",
    "\n",
    "**Y = [5, -12, 2]ᵀ**\n",
    "\n",
    "### Step 3: Solve U × X = Y for X using backward substitution\n",
    "\n",
    "**System:**\n",
    "```\n",
    "2x + 1y + 1z = 5\n",
    "0x - 8y - 2z = -12\n",
    "0x + 0y + 1z = 2\n",
    "```\n",
    "\n",
    "**Backward substitution:**\n",
    "- z = 2/1 = 2\n",
    "- y = (-12 - (-2×2))/(-8) = (-12 + 4)/(-8) = (-8)/(-8) = 1\n",
    "- x = (5 - 1×1 - 1×2)/2 = (5-1-2)/2 = 2/2 = 1\n",
    "\n",
    "**Solution: x = 1, y = 1, z = 2**\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Use Cases in Data Science\n",
    "\n",
    "1. **Linear Regression**: Efficiently solve normal equations (XᵀXβ = Xᵀy) for multiple right-hand sides\n",
    "2. **Matrix Inversion**: Foundation for computational methods to find A⁻¹\n",
    "3. **Determinant Calculation**: det(A) = det(L) × det(U) = product of U's diagonal elements\n",
    "4. **Eigenvalue Algorithms**: Used as a building block in numerical methods\n",
    "5. **Time Series Analysis**: Solving systems in ARIMA and state-space models\n",
    "6. **Image Processing**: Solving large systems in convolutional operations and deblurring\n",
    "7. **Optimization Problems**: Solving KKT conditions in constrained optimization\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Benefits and Limitations\n",
    "\n",
    "### Benefits:\n",
    "- **Efficiency**: Once decomposed, solving with different B vectors is O(n²) instead of O(n³)\n",
    "- **Numerical Stability**: With pivoting, it's more stable than Gaussian elimination\n",
    "- **Memory Efficient**: Stores decomposition in place of original matrix\n",
    "- **Multi-purpose**: Useful for determinant, inverse, and solving equations\n",
    "\n",
    "### Limitations:\n",
    "- **Only for Square Matrices**: Works only with square coefficient matrices\n",
    "- **Pivoting Required**: For numerical stability, especially for nearly singular matrices\n",
    "- **Not for All Matrices**: Some matrices require permutation (PA = LU)\n",
    "- **Dense Matrices**: For very large sparse systems, other methods may be better\n",
    "\n",
    "---\n",
    "\n",
    "## 6. When to Use LU Decomposition vs Other Methods\n",
    "\n",
    "| Method | Best For | When to Choose Over LU |\n",
    "|--------|----------|------------------------|\n",
    "| **LU Decomposition** | Multiple systems with same A | Default choice for medium-sized systems |\n",
    "| **Gaussian Elimination** | Single systems, teaching concepts | When you only need to solve one system |\n",
    "| **Cholesky Decomposition** | Symmetric positive definite matrices | When A is symmetric positive definite (faster) |\n",
    "| **QR Decomposition** | Least squares problems | For overdetermined systems (AX = B, A not square) |\n",
    "| **SVD** | Rank-deficient matrices | When A is singular or nearly singular |\n",
    "| **Iterative Methods** | Very large sparse systems | When A is too large for direct methods |\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Implementation Tips\n",
    "\n",
    "1. **Always Use Pivoting**: Implement partial pivoting (PA = LU) for numerical stability\n",
    "2. **Check Condition Number**: Verify matrix is well-conditioned before decomposition\n",
    "3. **Sparse Matrices**: Use specialized algorithms (like sparse LU) for sparse systems\n",
    "4. **Verification**: Always check that L × U ≈ original matrix\n",
    "5. **Memory Management**: For large matrices, use in-place computation to save memory\n",
    "6. **Parallelization**: Consider parallel LU implementations for very large systems\n",
    "\n",
    "\n",
    "---\n",
    "## 8. Python Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b42f9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [1. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Python implementation example\n",
    "import numpy as np\n",
    "from scipy.linalg import lu, solve\n",
    "\n",
    "# Create matrices\n",
    "A = np.array([[2, 1, 1], [4, -6, 0], [-2, 7, 2]])\n",
    "B = np.array([5, -2, 9])\n",
    "\n",
    "# LU decomposition with pivoting\n",
    "P, L, U = lu(A)\n",
    "\n",
    "# Solve system\n",
    "y = solve(L, P @ B)\n",
    "x = solve(U, y)\n",
    "\n",
    "print(\"Solution:\", x)\n",
    "# Output: [1. 1. 2.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fba932",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8. Singular Value Decomposition (SVD)\n",
    "\n",
    "## 1. Simple Definition\n",
    "\n",
    "Singular Value Decomposition (SVD) is a matrix factorization method that decomposes any matrix (square or rectangular) into three matrices:\n",
    "- **U**: Left singular vectors (orthogonal matrix)\n",
    "- **Σ**: Diagonal matrix of singular values (non-negative, in descending order)\n",
    "- **Vᵀ**: Right singular vectors (orthogonal matrix)\n",
    "\n",
    "Such that: **A = U × Σ × Vᵀ**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Explanation\n",
    "\n",
    "SVD is one of the most important matrix decompositions in linear algebra with wide applications in data science and machine learning:\n",
    "\n",
    "- Works for any matrix (square, rectangular, real, or complex)\n",
    "- Reveals the fundamental geometric structure of a matrix\n",
    "- Provides optimal low-rank approximations of matrices\n",
    "- Forms the mathematical foundation for many dimensionality reduction techniques\n",
    "- Is numerically stable and robust to computational errors\n",
    "\n",
    "The decomposition exists for any m×n matrix A, where:\n",
    "- U is an m×m orthogonal matrix (UᵀU = I)\n",
    "- Σ is an m×n diagonal matrix with non-negative entries\n",
    "- V is an n×n orthogonal matrix (VᵀV = I)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Example: Solving a System Using SVD (Mathematical Approach)\n",
    "\n",
    "**System of Equations:**\n",
    "```\n",
    "x + 2y + 3z = 14\n",
    "4x + 5y + 6z = 32\n",
    "7x + 8y + 10z = 53\n",
    "```\n",
    "\n",
    "**Matrix form: A × X = B**\n",
    "```\n",
    "A = [[1, 2, 3],\n",
    "     [4, 5, 6],\n",
    "     [7, 8, 10]]\n",
    "     \n",
    "B = [[14],\n",
    "     [32],\n",
    "     [53]]\n",
    "```\n",
    "\n",
    "### Step 1: Compute AᵀA and AAᵀ\n",
    "\n",
    "First, we compute the products:\n",
    "```\n",
    "AᵀA = [[1,4,7],    [[1,2,3],    [[66,  78,  97],\n",
    "        [2,5,8],  ×  [4,5,6],  =  [78,  93, 116],\n",
    "        [3,6,10]]    [7,8,10]]    [97, 116, 145]]\n",
    "        \n",
    "AAᵀ = [[1,2,3],    [[1,4,7],    [[14, 32,  50],\n",
    "        [4,5,6],  ×  [2,5,8],  =  [32, 77, 122],\n",
    "        [7,8,10]]    [3,6,10]]    [50,122,193]]\n",
    "```\n",
    "\n",
    "### Step 2: Find Eigenvalues and Eigenvectors of AᵀA\n",
    "\n",
    "Solve det(AᵀA - λI) = 0:\n",
    "```\n",
    "det([[66-λ, 78, 97],\n",
    "     [78, 93-λ, 116],\n",
    "     [97, 116, 145-λ]]) = 0\n",
    "```\n",
    "\n",
    "After calculation, we get eigenvalues:\n",
    "λ₁ ≈ 283.5, λ₂ ≈ 18.5, λ₃ ≈ 1.0\n",
    "\n",
    "Singular values: σ₁ = √λ₁ ≈ 16.84, σ₂ = √λ₂ ≈ 4.30, σ₃ = √λ₃ ≈ 1.00\n",
    "\n",
    "Eigenvectors (normalized):\n",
    "v₁ ≈ [-0.46, -0.57, -0.68]ᵀ\n",
    "v₂ ≈ [0.79, 0.16, -0.60]ᵀ\n",
    "v₃ ≈ [0.41, -0.81, 0.43]ᵀ\n",
    "\n",
    "Thus, V = [v₁, v₂, v₃]\n",
    "\n",
    "### Step 3: Find U Matrix\n",
    "\n",
    "Compute U columns using uᵢ = (1/σᵢ)Avᵢ:\n",
    "```\n",
    "u₁ = (1/16.84)A×v₁ ≈ [-0.21, -0.52, -0.83]ᵀ\n",
    "u₂ = (1/4.30)A×v₂ ≈ [0.76, 0.43, -0.49]ᵀ\n",
    "u₃ = (1/1.00)A×v₃ ≈ [0.62, -0.74, 0.27]ᵀ\n",
    "```\n",
    "\n",
    "Thus, U = [u₁, u₂, u₃]\n",
    "\n",
    "### Step 4: Compute Pseudoinverse A⁺\n",
    "\n",
    "A⁺ = VΣ⁺Uᵀ, where Σ⁺ is the pseudoinverse of Σ (reciprocals of non-zero singular values):\n",
    "```\n",
    "Σ⁺ = [[1/16.84, 0, 0],\n",
    "      [0, 1/4.30, 0],\n",
    "      [0, 0, 1/1.00]]\n",
    "```\n",
    "\n",
    "### Step 5: Solve for X\n",
    "\n",
    "X = A⁺B = VΣ⁺UᵀB\n",
    "\n",
    "After matrix multiplication:\n",
    "```\n",
    "X ≈ [[1],\n",
    "     [2],\n",
    "     [3]]\n",
    "```\n",
    "\n",
    "**Solution: x = 1, y = 2, z = 3**\n",
    "\n",
    "Verification:\n",
    "```\n",
    "1(1) + 2(2) + 3(3) = 1 + 4 + 9 = 14 ✓\n",
    "4(1) + 5(2) + 6(3) = 4 + 10 + 18 = 32 ✓\n",
    "7(1) + 8(2) + 10(3) = 7 + 16 + 30 = 53 ✓\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Use Cases in Data Science\n",
    "\n",
    "1. **Dimensionality Reduction (PCA)**: SVD is the computational foundation for Principal Component Analysis\n",
    "2. **Recommendation Systems**: Collaborative filtering using matrix factorization\n",
    "3. **Image Compression**: Representing images with fewer components\n",
    "4. **Natural Language Processing**: Latent Semantic Analysis (LSA) for document retrieval\n",
    "5. **Data Denoising**: Removing noise by truncating small singular values\n",
    "6. **Inverse Problems**: Solving ill-conditioned systems in physics and engineering\n",
    "7. **Computer Vision**: Structure from motion and facial recognition\n",
    "8. **Signal Processing**: Separating mixed signals (blind source separation)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Benefits and Limitations\n",
    "\n",
    "### Benefits:\n",
    "- **Universal Applicability**: Works for any matrix (square, rectangular, rank-deficient)\n",
    "- **Numerical Stability**: More stable than other decompositions for ill-conditioned matrices\n",
    "- **Optimal Approximations**: Provides the best low-rank approximation of a matrix (Eckart-Young theorem)\n",
    "- **Reveals Structure**: Shows the intrinsic geometry of the data\n",
    "- **Robust to Noise**: Truncating small singular values can remove noise\n",
    "\n",
    "### Limitations:\n",
    "- **Computational Cost**: O(mn²) for m×n matrix where m ≥ n\n",
    "- **Memory Intensive**: For very large matrices\n",
    "- **Interpretation**: Singular vectors may not have direct interpretation in original domain\n",
    "- **Implementation Complexity**: More complex to implement than simpler decompositions\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Python Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac1a9685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create the matrix and vector\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 10]])\n",
    "B = np.array([14, 32, 53])\n",
    "\n",
    "# Compute SVD\n",
    "U, S, Vt = np.linalg.svd(A)\n",
    "\n",
    "# Create Sigma matrix with proper dimensions\n",
    "Sigma = np.zeros(A.shape)\n",
    "Sigma[:len(S), :len(S)] = np.diag(S)\n",
    "\n",
    "# Compute pseudoinverse\n",
    "Sigma_plus = np.zeros(A.shape).T\n",
    "Sigma_plus[:len(S), :len(S)] = np.diag(1/S)\n",
    "A_plus = Vt.T @ Sigma_plus @ U.T\n",
    "\n",
    "# Solve the system\n",
    "X = A_plus @ B\n",
    "\n",
    "print(\"Solution:\", X)\n",
    "# Output: [1. 2. 3.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f2315b",
   "metadata": {},
   "source": [
    "> ### __For large matrices, use truncated SVD__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4dfa49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (1000, 500)\n",
      "Reduced shape: (1000, 50)\n",
      "Explained variance ratio: 0.23303124749975593\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Create a large matrix (example)\n",
    "large_matrix = np.random.rand(1000, 500)\n",
    "\n",
    "# Apply truncated SVD\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "reduced_data = svd.fit_transform(large_matrix)\n",
    "\n",
    "print(\"Original shape:\", large_matrix.shape)\n",
    "print(\"Reduced shape:\", reduced_data.shape)\n",
    "print(\"Explained variance ratio:\", svd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b32c9fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. When to Use SVD vs Other Methods\n",
    "\n",
    "| Method | Best For | When to Choose Over SVD |\n",
    "|--------|----------|------------------------|\n",
    "| **SVD** | General-purpose, rank-deficient matrices, dimensionality reduction | Default for robust solutions and dimensionality reduction |\n",
    "| **LU Decomposition** | Square systems with unique solutions | When A is square and well-conditioned (faster) |\n",
    "| **QR Decomposition** | Least squares problems | When A has full column rank (more efficient) |\n",
    "| **Cholesky Decomposition** | Symmetric positive definite matrices | When A is symmetric positive definite (much faster) |\n",
    "| **Eigen Decomposition** | Square symmetric matrices | When A is symmetric and you need eigenvalues |\n",
    "\n",
    "**Choose SVD when:**\n",
    "- The matrix is rectangular or rank-deficient\n",
    "- You need the most numerically stable solution\n",
    "- You want to understand the fundamental structure of the data\n",
    "- You need low-rank approximations for compression or denoising\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Implementation Tips\n",
    "\n",
    "1. **Use Established Libraries**: Prefer numpy.linalg.svd or scipy.linalg.svd over custom implementations\n",
    "2. **Truncation for Efficiency**: Use truncated SVD (e.g., sklearn.decomposition.TruncatedSVD) for large matrices\n",
    "3. **Randomized SVD**: For very large matrices, consider randomized SVD algorithms\n",
    "4. **Condition Number**: Check the condition number (σ_max/σ_min) to understand stability\n",
    "5. **Regularization**: For ill-conditioned problems, add regularization (Tikhonov regularization)\n",
    "6. **Memory Management**: For massive matrices, use iterative methods or out-of-core computation\n",
    "\n",
    "## 9. Conclusion\n",
    "\n",
    "SVD is a fundamental tool in the data scientist's toolkit, offering unparalleled versatility for matrix analysis and decomposition. Its ability to handle any matrix type, provide optimal low-rank approximations, and reveal the intrinsic structure of data makes it invaluable for tasks ranging from dimensionality reduction to solving ill-conditioned systems.\n",
    "\n",
    "While computationally more expensive than some alternatives, its numerical stability and theoretical foundations make it the preferred choice for many advanced data science applications. Understanding when and how to apply SVD—including its truncated variants for large-scale problems—is essential for modern data analysis and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ebb87b",
   "metadata": {},
   "source": [
    "# 9. Iterative Methods for Solving Linear Systems\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "Iterative methods are algorithms used to solve systems of linear equations by repeatedly improving approximations to the solution. Unlike direct methods (e.g., Gaussian elimination), which provide an exact solution in a finite number of steps, iterative methods start with an initial guess and refine it until convergence. They are particularly useful for large, sparse systems where direct methods are computationally expensive.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Simple Definition\n",
    "An iterative method solves \\( A\\mathbf{x} = \\mathbf{b} \\) by generating a sequence of approximations \\( \\mathbf{x}^{(0)}, \\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots \\) that converge to the exact solution. Common techniques include the **Jacobi method** and **Gauss-Seidel method**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Explanation\n",
    "\n",
    "### - General Idea:\n",
    "1. Start with an initial guess \\( \\mathbf{x}^{(0)} \\).\n",
    "2. Update each variable using the current values of other variables.\n",
    "3. Repeat until the change between iterations is below a tolerance threshold.\n",
    "\n",
    "### - Jacobi Method:\n",
    "- Each variable is updated using values from the *previous* iteration.\n",
    "- Formula for the \\( k \\)-th iteration:\n",
    "  \\[\n",
    "  x_i^{(k)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j \\neq i} a_{ij} x_j^{(k-1)} \\right)\n",
    "  \\]\n",
    "\n",
    "### - Gauss-Seidel Method:\n",
    "- Uses the *latest* updated values in the same iteration.\n",
    "- Typically converges faster than Jacobi.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Example: Solving a 3×3 System\n",
    "### Problem:\n",
    "Solve the system:\n",
    "\\[\n",
    "\\begin{align*}\n",
    "4x + y + z &= 7 \\\\\n",
    "x + 3y + z &= 5 \\\\\n",
    "x + y + 5z &= 3 \\\\\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "### Step-by-Step Solution using Jacobi Method:\n",
    "1. **Rearrange equations to isolate variables:**\n",
    "   \\[\n",
    "   x = \\frac{7 - y - z}{4}, \\quad y = \\frac{5 - x - z}{3}, \\quad z = \\frac{3 - x - y}{5}\n",
    "   \\]\n",
    "\n",
    "2. **Initial guess:** \\( x^{(0)} = 0, y^{(0)} = 0, z^{(0)} = 0 \\).\n",
    "\n",
    "3. **Iteration 1:**\n",
    "   \\[\n",
    "   x^{(1)} = \\frac{7 - 0 - 0}{4} = 1.75 \\\\\n",
    "   y^{(1)} = \\frac{5 - 0 - 0}{3} \\approx 1.6667 \\\\\n",
    "   z^{(1)} = \\frac{3 - 0 - 0}{5} = 0.6\n",
    "   \\]\n",
    "\n",
    "4. **Iteration 2:**\n",
    "   \\[\n",
    "   x^{(2)} = \\frac{7 - 1.6667 - 0.6}{4} \\approx 1.1833 \\\\\n",
    "   y^{(2)} = \\frac{5 - 1.75 - 0.6}{3} \\approx 0.8833 \\\\\n",
    "   z^{(2)} = \\frac{3 - 1.75 - 1.6667}{5} \\approx -0.0833\n",
    "   \\]\n",
    "\n",
    "5. **Iteration 3:**\n",
    "   \\[\n",
    "   x^{(3)} = \\frac{7 - 0.8833 - (-0.0833)}{4} \\approx 1.55 \\\\\n",
    "   y^{(3)} = \\frac{5 - 1.1833 - (-0.0833)}{3} \\approx 1.3 \\\\\n",
    "   z^{(3)} = \\frac{3 - 1.1833 - 0.8833}{5} \\approx 0.1867\n",
    "   \\]\n",
    "\n",
    "6. **Continue until convergence** (e.g., after 10 iterations):\n",
    "   \\[\n",
    "   x \\approx 1.0, \\quad y \\approx 1.0, \\quad z \\approx 0.0\n",
    "   \\]\n",
    "\n",
    "**Exact solution:** \\( x = 1, y = 1, z = 0 \\).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Use Cases in Data Science\n",
    "1. **Large-Scale Linear Systems:** Solving normal equations in linear regression for big datasets.\n",
    "2. **PageRank Algorithm:** Used by Google to rank web pages (solving a massive linear system iteratively).\n",
    "3. **Image Reconstruction:** In tomography and deblurring.\n",
    "4. **Machine Learning:** Training linear models (e.g., SGD for optimization).\n",
    "5. **Finite Element Methods:** Solving partial differential equations numerically.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Benefits and Limitations\n",
    "### - Benefits:\n",
    "- **Efficiency for Large Systems:** Reduced memory and computation time for sparse matrices.\n",
    "- **Parallelization:** Jacobi method can be parallelized easily.\n",
    "- **Simplicity:** Easy to implement and understand.\n",
    "\n",
    "### - Limitations:\n",
    "- **Convergence Not Guaranteed:** Requires diagonal dominance or symmetry for convergence.\n",
    "- **Slow Convergence:** May need many iterations for high accuracy.\n",
    "- **Accuracy:** Provides approximate solutions.\n",
    "\n",
    "---\n",
    "\n",
    "> ## 7. Python Implementation\n",
    "### Jacobi Method Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022467b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def jacobi(A, b, tol=1e-10, max_iterations=100):\n",
    "    n = len(b)\n",
    "    x = np.zeros_like(b)\n",
    "    for k in range(max_iterations):\n",
    "        x_new = np.zeros_like(x)\n",
    "        for i in range(n):\n",
    "            s = np.dot(A[i, :], x) - A[i, i] * x[i]\n",
    "            x_new[i] = (b[i] - s) / A[i, i]\n",
    "        if np.linalg.norm(x_new - x) < tol:\n",
    "            return x_new\n",
    "        x = x_new\n",
    "    return x\n",
    "\n",
    "# Example system:\n",
    "A = np.array([[4, 1, 1], [1, 3, 1], [1, 1, 5]])\n",
    "b = np.array([7, 5, 3])\n",
    "x = jacobi(A, b)\n",
    "print(\"Solution:\", x)\n",
    "\n",
    "### Output: [1. 1. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99535b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. When to Use Iterative Methods\n",
    "1. **Large Sparse Matrices:** When \\( A \\) has mostly zeros (e.g., network data).\n",
    "2. **Memory Constraints:** Direct methods require storing entire matrices, while iterative methods use less memory.\n",
    "3. **Approximate Solutions Needed:** When an approximate solution is acceptable (e.g., machine learning).\n",
    "\n",
    "**Avoid iterative methods for:**\n",
    "- Small or dense systems.\n",
    "- Ill-conditioned matrices (unless preconditioned).\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Tips\n",
    "1. **Check Diagonal Dominance:** Ensure \\( |a_{ii}| > \\sum_{j \\neq i} |a_{ij}| \\) for convergence.\n",
    "2. **Preconditioning:** Use preconditioners (e.g., Jacobi preconditioner) to speed up convergence.\n",
    "3. **Initial Guess:** Choose a good initial guess (e.g., from a similar problem) to reduce iterations.\n",
    "4. **Stop Early:** In ML, early stopping can prevent overfitting and save time.\n",
    "5. **Hybrid Approaches:** Combine direct and iterative methods (e.g., use direct method for preconditioning).\n",
    "\n",
    "---\n",
    "\n",
    "## 10. References\n",
    "- Golub, G. H., & Van Loan, C. F. (2013). *Matrix Computations*.\n",
    "- Saad, Y. (2003). *Iterative Methods for Sparse Linear Systems*.\n",
    "\n",
    "**Note:** Always validate results with direct methods for critical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e58a3da",
   "metadata": {},
   "source": [
    "# 10. Cramer's Rule: Matrix Solving Method\n",
    "\n",
    "## 1. Definition\n",
    "Cramer's Rule is a method for solving systems of linear equations using determinants. It expresses the solution in terms of the ratio of determinants of matrices derived from the coefficient matrix and the constant terms.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Explanation\n",
    "For a system of linear equations with n variables, Cramer's Rule states that:\n",
    "- If the coefficient matrix has a non-zero determinant, the system has a unique solution\n",
    "- Each variable can be found by replacing the corresponding column in the coefficient matrix with the column of constants and computing the ratio of determinants\n",
    "\n",
    "The formula for each variable is:\n",
    "```\n",
    "xᵢ = Det(Aᵢ)/Det(A)\n",
    "```\n",
    "where:\n",
    "- A is the coefficient matrix\n",
    "- Aᵢ is the matrix formed by replacing the ith column of A with the constants column\n",
    "- Det() represents the determinant\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Example: Solving a 3×3 System\n",
    "\n",
    "Consider the system:\n",
    "```\n",
    "2x + y + z = 8\n",
    "x - 3y + z = -2\n",
    "4x + y - 2z = 4\n",
    "```\n",
    "\n",
    "### Step 1: Write in matrix form\n",
    "A = [\n",
    "    [2, 1, 1],\n",
    "    [1, -3, 1],\n",
    "    [4, 1, -2]\n",
    "]\n",
    "\n",
    "b = [8, -2, 4]\n",
    "\n",
    "### Step 2: Calculate Det(A)\n",
    "Det(A) = 2·(-3)·(-2) + 1·1·4 + 1·1·1 - 1·(-3)·4 - 2·1·1 - 1·1·(-2)\n",
    "       = 12 + 4 + 1 - (-12) - 2 - (-2)\n",
    "       = 12 + 4 + 1 + 12 - 2 + 2\n",
    "       = 29\n",
    "\n",
    "### Step 3: Calculate Det(A₁)\n",
    "A₁ = [\n",
    "    [8, 1, 1],\n",
    "    [-2, -3, 1],\n",
    "    [4, 1, -2]\n",
    "]\n",
    "\n",
    "Det(A₁) = 8·(-3)·(-2) + 1·1·4 + 1·(-2)·(-2) - 1·(-3)·4 - 8·1·(-2) - (-2)·1·1\n",
    "        = 48 + 4 + 4 - (-12) - (-16) - (-2)\n",
    "        = 48 + 4 + 4 + 12 + 16 + 2\n",
    "        = 86\n",
    "\n",
    "### Step 4: Calculate Det(A₂)\n",
    "A₂ = [\n",
    "    [2, 8, 1],\n",
    "    [1, -2, 1],\n",
    "    [4, 4, -2]\n",
    "]\n",
    "\n",
    "Det(A₂) = 2·(-2)·(-2) + 8·1·4 + 1·1·4 - 1·(-2)·4 - 2·1·4 - 8·1·(-2)\n",
    "        = 8 + 32 + 4 - (-8) - 8 - (-16)\n",
    "        = 8 + 32 + 4 + 8 - 8 + 16\n",
    "        = 60\n",
    "\n",
    "### Step 5: Calculate Det(A₃)\n",
    "A₃ = [\n",
    "    [2, 1, 8],\n",
    "    [1, -3, -2],\n",
    "    [4, 1, 4]\n",
    "]\n",
    "\n",
    "Det(A₃) = 2·(-3)·4 + 1·(-2)·4 + 8·1·1 - 8·(-3)·4 - 2·(-2)·1 - 1·1·4\n",
    "        = -24 - 8 + 8 - (-96) - (-4) - 4\n",
    "        = -24 - 8 + 8 + 96 + 4 - 4\n",
    "        = 72\n",
    "\n",
    "### Step 6: Calculate the solutions\n",
    "x = Det(A₁)/Det(A) = 86/29 ≈ 2.97\n",
    "y = Det(A₂)/Det(A) = 60/29 ≈ 2.07\n",
    "z = Det(A₃)/Det(A) = 72/29 ≈ 2.48\n",
    "\n",
    "Therefore, x ≈ 3, y ≈ 2, z ≈ 2.5\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Use Cases in Data Science\n",
    "1. **Solving regression equations** when working with small to medium datasets\n",
    "2. **Feature extraction** when solving systems of equations in statistical models\n",
    "3. **Computer graphics** for transformations and calculations\n",
    "4. **Network analysis** when solving flow equations\n",
    "5. **Finance models** for portfolio optimization equations\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Benefits\n",
    "1. **Direct formula**: Provides an explicit formula for each variable\n",
    "2. **Theoretical value**: Helps understand the structure of linear systems\n",
    "3. **No need for elimination steps**: Unlike Gaussian elimination, variables are computed directly\n",
    "4. **Good for small systems**: Very clear and straightforward for 2×2 and 3×3 systems\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Limitations\n",
    "1. **Computational complexity**: O(n!) time complexity makes it inefficient for large systems\n",
    "2. **Numerical instability**: Can suffer from round-off errors\n",
    "3. **Cannot handle singular matrices**: If Det(A) = 0, Cramer's Rule cannot be applied\n",
    "4. **Not suitable for sparse matrices**: Doesn't take advantage of sparsity\n",
    "\n",
    "---\n",
    "\n",
    "## 7. When to Use Cramer's Rule vs. Other Methods\n",
    "- **Use Cramer's Rule when**:\n",
    "  - Working with small systems (2×2, 3×3)\n",
    "  - Teaching or explaining linear systems\n",
    "  - Theoretical proofs\n",
    "  - Checking solutions obtained by other methods\n",
    "\n",
    "- **Use other methods when**:\n",
    "  - Working with large systems (use Gaussian elimination)\n",
    "  - Dealing with sparse matrices (use specialized sparse solvers)\n",
    "  - Need higher numerical stability (use LU decomposition)\n",
    "  - Iterative solutions required (use Jacobi or Gauss-Seidel)\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Python Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab80e75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution using Cramer's Rule:\n",
      "x = 1.7241379310344815\n",
      "y = 2.0689655172413786\n",
      "z = 2.482758620689655\n",
      "\n",
      "Verification:\n",
      "Ax = [ 8. -2.  4.]\n",
      "b = [ 8 -2  4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cramers_rule(A, b):\n",
    "    \"\"\"\n",
    "    Solve a system of linear equations using Cramer's Rule.\n",
    "    \n",
    "    Parameters:\n",
    "    A (numpy.ndarray): Coefficient matrix\n",
    "    b (numpy.ndarray): Constants vector\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Solution vector\n",
    "    \"\"\"\n",
    "    # Get the number of variables/equations\n",
    "    n = len(b)\n",
    "    \n",
    "    # Calculate the determinant of the coefficient matrix\n",
    "    det_A = np.linalg.det(A)\n",
    "    \n",
    "    # Check if the determinant is zero\n",
    "    if abs(det_A) < 1e-10:\n",
    "        raise ValueError(\"The coefficient matrix is singular, Cramer's Rule cannot be applied.\")\n",
    "    \n",
    "    # Initialize solution vector\n",
    "    x = np.zeros(n)\n",
    "    \n",
    "    # Apply Cramer's Rule for each variable\n",
    "    for i in range(n):\n",
    "        # Create a copy of A\n",
    "        A_i = A.copy()\n",
    "        # Replace the i-th column with the constants\n",
    "        A_i[:, i] = b\n",
    "        # Calculate the determinant\n",
    "        det_A_i = np.linalg.det(A_i)\n",
    "        # Calculate the value of the i-th variable\n",
    "        x[i] = det_A_i / det_A\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Example usage\n",
    "A = np.array([[2, 1, 1], \n",
    "              [1, -3, 1], \n",
    "              [4, 1, -2]])\n",
    "b = np.array([8, -2, 4])\n",
    "\n",
    "try:\n",
    "    solution = cramers_rule(A, b)\n",
    "    print(\"Solution using Cramer's Rule:\")\n",
    "    print(f\"x = {solution[0]}\")\n",
    "    print(f\"y = {solution[1]}\")\n",
    "    print(f\"z = {solution[2]}\")\n",
    "    \n",
    "    # Verify the solution\n",
    "    print(\"\\nVerification:\")\n",
    "    print(f\"Ax = {np.dot(A, solution)}\")\n",
    "    print(f\"b = {b}\")\n",
    "    \n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed9577",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Tips for Using Cramer's Rule\n",
    "\n",
    "1. **Check the determinant first**: If Det(A) is zero or very close to zero, don't use Cramer's Rule.\n",
    "\n",
    "2. **Use it as a teaching tool**: It's excellent for understanding the relationship between determinants and linear systems.\n",
    "\n",
    "3. **Consider computational efficiency**: For systems larger than 3×3, other methods are typically more efficient.\n",
    "\n",
    "4. **Symbolic computation**: Cramer's Rule works well with symbolic math libraries when exact solutions are needed.\n",
    "\n",
    "5. **Double-check your work**: Verify solutions by substituting back into the original equations.\n",
    "\n",
    "6. **Benchmark against other methods**: Compare solutions with NumPy's built-in solver for accuracy validation.\n",
    "\n",
    "7. **Don't reinvent the wheel**: For practical applications in data science, use optimized libraries like NumPy's `linalg.solve()` instead of implementing Cramer's Rule manually.\n",
    "\n",
    "8. **Remember its applications**: Though not always computationally efficient, Cramer's Rule has important theoretical applications in linear algebra, differential equations, and circuit analysis.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
